# Applying AI Techniques in Networking: A Study on the Efficiency of BBO for the MLP

---

## ğŸ“Œ Project Description

This project investigates three core objectives:

1. **How BBO performance changes under 10 different parameter configurations** across various MLP instances.
2. **How the best-case MLP tour cost from BBO compares to ACO and GA**.
3. **How the best-case computation time from BBO compares to ACO and GA**.

To accomplish this, the project performs:

- Multiple BBO runs per TSP/MLP instance across **10 parameter configurations**  
- **Shapiroâ€“Wilk tests** to determine data normality  
- **ANOVA and Mannâ€“Whitney U (MWU) tests** for statistical significance  
- **Pareto-front analysis** to identify the *best-case* BBO performance  
- Comparative statistical analysis between **BBO vs ACO vs GA**

---

## ğŸ“‚ Repository Structure
â”œâ”€â”€ data/ â†’ All .tsp MLP instances (lin105, lin318, pr107, etc.)
â”‚
â”œâ”€â”€ BBO-Parameter-Test/ â†’ BBO runs on each MLP instance
â”‚ â”œâ”€â”€ *.ipynb Each file = 10 BBO parameter configurations
â”‚ Output: cost + computation time per run
â”‚
â”œâ”€â”€ Shapiro-Wilk-Test-All/ â†’ Normality tests for all BBO runs (cost + time)
â”‚ â””â”€â”€ Shapiro-Wilk-Test-All.ipynb
â”‚
â”œâ”€â”€ ANOVA-MWU-Test-Individual/ â†’ Statistical tests for RQ1
â”‚ â”œâ”€â”€ Table 22.ipynb
â”‚ â”œâ”€â”€ Table 23.ipynb
â”‚ â””â”€â”€ ... Table 30.ipynb Uses ANOVA or MWU depending on normality
â”‚
â”œâ”€â”€ Pareto-Front-Test-All/ â†’ Identifies best-case BBO solutions
â”‚ â””â”€â”€ Table 31_ Summary of Pareto-Front analysis.ipynb
â”‚
â”œâ”€â”€ Shapiro-Wilk-BBOvsACO/ â†’ Normality tests for RQ2 & RQ3 (BBO vs ACO)
â”‚ â””â”€â”€ Table 36.ipynb
â”‚
â”œâ”€â”€ Shapiro-Wilk-BBOvsGA/ â†’ Normality tests for RQ2 & RQ3 (BBO vs GA)
â”‚ â””â”€â”€ Table 33.ipynb
â”‚
â”œâ”€â”€ MWU-BBOvsACO/ â†’ MWU comparison between BBO and ACO
â”‚ â””â”€â”€ Table 37.ipynb
â”‚
â”œâ”€â”€ MWU-BBOvsGA/ â†’ MWU comparison between BBO and GA
â”‚ â””â”€â”€ Table 34.ipynb
â”‚
â””â”€â”€ README.md

---

## ğŸ¯ Research Questions and Hypotheses

### **RQ1: Effect of BBO Parameter Configurations on MLP**
**H0:** BBO performance does not significantly differ across parameter configurations.  
**H1:** At least one parameter configuration produces significantly different performance.

**Result:**  
H0 accepted for nearly all TSP instances **except pr107.tsp (cost)** where H1 is accepted.

---

### **RQ2: Best-Case MLP Tour Cost â€” BBO vs ACO vs GA**
**H0:** No significant difference in best-case cost among BBO, ACO, and GA.  
**H1:** At least one algorithm produces significantly different best-case cost.

**Result:**  
MWU test found **no significant difference** â€” H0 accepted.

---

### **RQ3: Best-Case Computation Time â€” BBO vs ACO vs GA**
**H0:** No significant difference in computation time.  
**H1:** There is a significant difference.

**Result:**  
MWU test found **significant differences in time** â€” H1 accepted (BBO differs from ACO and GA).

---

## ğŸ§ª Methodology Overview

### **Step 1 â€” Dataset Preparation**
All `.tsp` files from TSPLIB95 stored in `/data`.

### **Step 2 â€” BBO Parameter Experiments**
- 1 notebook per MLP instance  
- 10 parameter configurations per notebook  
- Outputs:
  - Tour cost  
  - Computation time  

### **Step 3 â€” Shapiroâ€“Wilk Normality Analysis**
Determines whether:
- ANOVA (parametric)
- Mannâ€“Whitney U (nonparametric)  
should be used.

### **Step 4 â€” ANOVA/MWU for RQ1**
Tests whether BBO performance differs across its 10 parameter settings.

### **Step 5 â€” Pareto-Front Determination**
Identifies **best-case cost and time** for each MLP instance.

These serve as the benchmark for RQ2 and RQ3.

### **Step 6â€“7 â€” Normality Tests for BBO vs ACO vs GA**
Normality tested separately for cost and time.

### **Step 8â€“9 â€” MWU Comparison Against ACO and GA**
Formal statistical verification of RQ2 and RQ3.

---

## âš™ï¸ Installation

### **Clone the repository** 

git clone https://github.com/WaterBear1234/UoY-Independent-Research-Project.git
cd UoY-Independent-Research-Project.git

### **Install dependencies** 

pip install numpy tsplib95 random math time pandas scipy matplotlib jupyter

---

## â–¶ï¸ How to Run the Project

### **Step 1 â€” Start Jupyter Notebook** 

jupyter notebook

### **Step 2 â€” Navigate to one of the project folders**

BBO-Parameter-Test/ to re-run BBO

Shapiro-Wilk-Test-All/ to reproduce normality tests

Pareto-Front-Test-All/ for best-case analysis

MWU-BBOvsACO/ or MWU-BBOvsGA/ for algorithm comparison

### **Step 3 â€” Run all cells sequentially** 

---

## ğŸ§ª How to Use the Repository

### **To test a new MLP instance** 

1. Place .tsp file in /data.

2. Create a new notebook in BBO-Parameter-Test/.

3. Set the parameter configurations.

4. Run 10 experiments and save the results.

5. Follow Steps 3â€“9 for statistical verification.

### **To compare against new algorithms** 

Repeat Steps 6â€“9 with the new algorithmâ€™s cost/time results.

---

## ğŸ‘¥ Credits

Developed by **Bui Hoang My Linh**

Supervised by **Dr. Johnson Eze** and **Dr. Bashir Dodo**

Based on MLP results from **Mafort & Ochi (ACO)** and **Ban & Nguyen (GA)**

---

## ğŸ“œ License

This project is licensed under the **MIT License**.
